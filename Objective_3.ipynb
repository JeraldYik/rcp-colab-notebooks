{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Objective 3",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeraldYik/rcp-colab-notebooks/blob/main/Objective_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzBMhO4ptET7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aade35b0-dd55-4e5c-b9cb-7876cedf5644"
      },
      "source": [
        "# Referred to article: https://medium.com/@yvettewu.dw/tutorial-kaggle-api-google-colaboratory-1a054a382de0\n",
        "!pip install kaggle --upgrade\n",
        "\n",
        "!rm -rf .kaggle\n",
        "!mkdir .kaggle\n",
        "import json\n",
        "username = \"jeraldyik\"\n",
        "key = \"889ee4e40f578ab82c7a7f9bbf1018dc\"\n",
        "token = {\"username\":username,\"key\":key}\n",
        "with open(\"/content/.kaggle/kaggle.json\", \"w\") as file:\n",
        "    json.dump(token, file)\n",
        "\n",
        "!chmod 600 /content/.kaggle/kaggle.json\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "!kaggle config set -n path -v{/content}\n",
        "\n",
        "!kaggle competitions download -c dogs-vs-cats -p /content\n",
        "\n",
        "!unzip test1.zip\n",
        "!unzip train.zip\n",
        "\n",
        "import os, cv2, itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install np_utils\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "!pip install sklearn\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "!pip install tensorflow-privacy\n",
        "# will explore other DP Optimisers like DP Adam in the near future\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentOptimizer\n",
        "from tensorflow_privacy.privacy.dp_query import gaussian_query"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.9)\n",
            "Requirement already satisfied, skipping upgrade: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "- path is now set to: {/content}\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4)\n",
            "sampleSubmission.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test1.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  test1.zip\n",
            "replace test1/1.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "Archive:  train.zip\n",
            "replace train/cat.0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "Requirement already satisfied: np_utils in /usr/local/lib/python3.6/dist-packages (0.5.12.1)\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.6/dist-packages (from np_utils) (1.18.5)\n",
            "Requirement already satisfied: future>=0.16 in /usr/local/lib/python3.6/dist-packages (from np_utils) (0.16.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.17.0)\n",
            "Requirement already satisfied: tensorflow-privacy in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from tensorflow-privacy) (1.4.1)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.6/dist-packages (from tensorflow-privacy) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy>=0.17->tensorflow-privacy) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgVllDfDEJH_"
      },
      "source": [
        "class Cloud:\n",
        "\n",
        "  def __init__(self, num_clients):\n",
        "    self.TRAIN_DIR = './train/'\n",
        "    self.TEST_DIR = './test1/'\n",
        "\n",
        "    self.ROWS = 64\n",
        "    self.COLS = 64\n",
        "    self.CHANNELS = 3\n",
        "    self.EPOCHS = 50 \n",
        "    self.BATCH_SIZE = 64\n",
        "    self.LEARNING_RATE = 0.25\n",
        "    self.L2_NORM_CLIP = 1.0e9\n",
        "    self.STDDEV = 0.0\n",
        "    self.num_clients = num_clients\n",
        "    self.classes = {0: 'Cats', 1: 'Dogs'}\n",
        "    self.model = None\n",
        "\n",
        "  def read_image(self, file_path):\n",
        "    #print(file_path)\n",
        "    img = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
        "    #print(img)\n",
        "    return cv2.resize(img, (self.ROWS, self.COLS), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "  def prep_data(self,images):\n",
        "    m = len(images)\n",
        "    n_x = self.ROWS*self.COLS*self.CHANNELS\n",
        "    \n",
        "    x = np.ndarray((m,self.ROWS,self.COLS,self.CHANNELS), dtype=np.uint8)\n",
        "    y = np.zeros((m,1))\n",
        "    print(\"x.shape is {}\".format(x.shape))\n",
        "    \n",
        "    for i,image_file in enumerate(images) :\n",
        "      image = self.read_image(image_file)\n",
        "      x[i,:] = np.squeeze(image.reshape((self.ROWS, self.COLS, self.CHANNELS)))\n",
        "      if 'dog' in image_file.lower() :\n",
        "        y[i,0] = 1\n",
        "      elif 'cat' in image_file.lower() :\n",
        "        y[i,0] = 0\n",
        "      else : # for test data\n",
        "        y[i,0] = image_file.split('/')[-1].split('.')[0]\n",
        "        \n",
        "      if i%5000 == 0 :\n",
        "        print(\"Proceed {} of {}\".format(i, m))\n",
        "      \n",
        "    return x,y\n",
        "\n",
        "\n",
        "  ''' only dealing with train data for now '''\n",
        "  def generate_data(self):\n",
        "    train_images = [self.TRAIN_DIR+i for i in os.listdir(self.TRAIN_DIR)]\n",
        "\n",
        "    x_train, y_train = self.prep_data(train_images)\n",
        "\n",
        "    x_train, y_train = shuffle(x_train, y_train)\n",
        "\n",
        "    ''' splitting train data into array with length=num_clients '''\n",
        "    x_train_arr = [None for _ in range(self.num_clients)]\n",
        "    y_train_arr = [None for _ in range(self.num_clients)]\n",
        "\n",
        "    for i in range(self.num_clients):\n",
        "      x_train_arr[i] = x_train[(len(x_train)//self.num_clients)*i:(len(x_train)//self.num_clients)*(i+1)]\n",
        "      y_train_arr[i] = y_train[(len(x_train)//self.num_clients)*i:(len(x_train)//self.num_clients)*(i+1)]\n",
        "\n",
        "    return x_train_arr, y_train_arr\n",
        "\n",
        "\n",
        "  def generate_model(self):\n",
        "    '''\n",
        "    We define the model as the instance of Sequential() and then just define the layers \n",
        "    (Conv2D, MaxPooling2D, Dropout, Dense, Sigmoid). \n",
        "    Our model architecture has 4 Convolutional layers followed by 1 Fully Connected Layer followed by a Sigmoid output.\n",
        "\n",
        "    Loss function used — categorical_crossentropy\n",
        "\n",
        "    Optimizer used — Adam\n",
        "    '''\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, (3,3), input_shape=(self.ROWS, self.COLS, self.CHANNELS), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Conv2D(128, (3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Conv2D(256, (3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Conv2D(512, (1,1), activation='relu'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Dense(units=120, activation='relu'))\n",
        "    model.add(Dense(units=2, activation='sigmoid'))\n",
        "\n",
        "    ''' DP SGD optimiser from tensorflow_privacy library \n",
        "        TODO: research on what value of l2_norm_clip to use\n",
        "        refer to: https://github.com/tensorflow/privacy/search?q=gaussian_query '''\n",
        "    dp_sum_query = gaussian_query.GaussianSumQuery(l2_norm_clip=self.L2_NORM_CLIP, stddev=self.STDDEV)\n",
        "    dpGradientDescentOptimizer = DPGradientDescentOptimizer(dp_sum_query=dp_sum_query, learning_rate=self.LEARNING_RATE)\n",
        "\n",
        "    # model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.compile(optimizer=dpGradientDescentOptimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    self.model = model\n",
        "\n",
        "  def send_model(self):\n",
        "    return self.model\n",
        "\n",
        "  ''' receive new weights from aggregator and set to model '''\n",
        "  def receive_and_save_weights(self, weights):\n",
        "    print('-- Saving weights... --')\n",
        "    self.model.set_weights(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el1KSE6yu7si"
      },
      "source": [
        "class Client:\n",
        "\n",
        "  def __init__(self, idx):\n",
        "    self.idx = idx\n",
        "    self.model = None\n",
        "    self.x_train = None\n",
        "    self.y_train = None\n",
        "    self.x_train_norm = None\n",
        "    self.y_train_one_hot = None\n",
        "    self.x_val_norm = None\n",
        "    self.y_val_one_hot = None\n",
        "  \n",
        "  def download_model(self,model):\n",
        "    self.model = model\n",
        "\n",
        "  def retrieve_model(self):\n",
        "    return self.model\n",
        "\n",
        "  def download_data(self,data):\n",
        "    self.x_train = data[0][self.idx]\n",
        "    self.y_train = data[1][self.idx]\n",
        "\n",
        "  def manipulate_data(self):\n",
        "    ''' split the dataset into the train (80%) and validation set (20%) '''\n",
        "    x_train, x_val, y_train, y_val = train_test_split(self.x_train, self.y_train, test_size=0.2, random_state=1)\n",
        "\n",
        "    print(\"Train shape: {}\".format(x_train.shape))\n",
        "    print(\"Train label shape: {}\".format(y_train.shape))\n",
        "    print(\"Validation shape: {}\".format(x_val.shape))\n",
        "    print(\"Validation label shape: {}\".format(y_val.shape))\n",
        "\n",
        "    ''' convert to OHE '''\n",
        "    self.y_train_one_hot = to_categorical(y_train)\n",
        "    print(\"y_train_one_hot.shape: {}\".format(self.y_train_one_hot.shape))\n",
        "\n",
        "    num_classes = self.y_train_one_hot.shape[1]\n",
        "    print(\"num_classes: {}\".format(num_classes))\n",
        "\n",
        "    self.y_val_one_hot = to_categorical(y_val)\n",
        "    print(\"y_val_one_hot.shape: {}\".format(self.y_val_one_hot.shape))\n",
        "\n",
        "    ''' normalise our training and validation set '''\n",
        "    self.x_train_norm = x_train / 255\n",
        "    self.x_val_norm = x_val / 255\n",
        "\n",
        "  ''' fit the model with training data '''\n",
        "  def train(self, epochs, batch_size):\n",
        "    self.model.fit(self.x_train_norm, self.y_train_one_hot, validation_data=(self.x_val_norm, self.y_val_one_hot), epochs=epochs, batch_size = batch_size)\n",
        "\n",
        "  def get_weights_from_model(self):\n",
        "    return self.model.get_weights()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Lb1StLYvA7E"
      },
      "source": [
        "class Aggregator:\n",
        "  def __init__(self, num_clients):\n",
        "    self.weights_from_clients = [None for _ in range(num_clients)]\n",
        "    self.generation = 0\n",
        "    self.num_clients = num_clients\n",
        "  \n",
        "  def get_weights_from_clients(self, client_idx, weights):\n",
        "    print('-- Received weights from Client {} --'.format(client_idx))\n",
        "    self.weights_from_clients[client_idx] = weights\n",
        "\n",
        "  def aggregate_weights(self):\n",
        "    print('--- Aggregating weights... ---')\n",
        "\n",
        "    ''' Populate return list with empty numpy arrays of appropriate size '''\n",
        "    new_weights = []\n",
        "    for layer in self.weights_from_clients[0]:\n",
        "      new_weights.append(np.zeros(shape=layer.shape))\n",
        "    \n",
        "    ''' Add value of weights from all clients at each layer '''\n",
        "    for client_weights in self.weights_from_clients:\n",
        "      for i, w in enumerate(client_weights):\n",
        "        new_weights[i] += w\n",
        "\n",
        "    ''' Find average value of weights at each layer '''\n",
        "    for layer in new_weights:\n",
        "      layer /= self.num_clients\n",
        "\n",
        "    return new_weights\n",
        "\n",
        "  def send_weights_to_cloud(self, cloud):\n",
        "    aggregated_weights = self.aggregate_weights()\n",
        "    cloud.receive_and_save_weights(aggregated_weights)\n",
        "    self.generation += 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VurJKyVJvIr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc61ecb5-7715-42d4-d231-5a548cb33821"
      },
      "source": [
        "print('--- Preparing Cloud ---')\n",
        "NUM_CLIENTS = 2\n",
        "cloud = Cloud(num_clients=NUM_CLIENTS)\n",
        "data = cloud.generate_data()\n",
        "cloud.generate_model()\n",
        "clients = [None for _ in range(cloud.num_clients)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Preparing Cloud ---\n",
            "x.shape is (25000, 64, 64, 3)\n",
            "Proceed 0 of 25000\n",
            "Proceed 5000 of 25000\n",
            "Proceed 10000 of 25000\n",
            "Proceed 15000 of 25000\n",
            "Proceed 20000 of 25000\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_15 (Conv2D)           (None, 62, 62, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 29, 29, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 2, 2, 512)         131584    \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 120)               245880    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 2)                 242       \n",
            "=================================================================\n",
            "Total params: 766,122\n",
            "Trainable params: 766,122\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upEVrV28vKLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c07b15-3a39-4d2e-a95b-21318d71cf68"
      },
      "source": [
        "print('--- Preparing Aggregator ---')\n",
        "aggregator = Aggregator(cloud.num_clients)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Preparing Aggregator ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjNLQSYJvLgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bfc0db-e515-4104-898a-397ef656a1c7"
      },
      "source": [
        "''' Populate clients array and save static data into clients '''\n",
        "for i in range(len(clients)):\n",
        "  clients[i] = Client(i)\n",
        "  clients[i].download_data(data)\n",
        "  clients[i].manipulate_data()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (10000, 64, 64, 3)\n",
            "Train label shape: (10000, 1)\n",
            "Validation shape: (2500, 64, 64, 3)\n",
            "Validation label shape: (2500, 1)\n",
            "y_train_one_hot.shape: (10000, 2)\n",
            "num_classes: 2\n",
            "y_val_one_hot.shape: (2500, 2)\n",
            "Train shape: (10000, 64, 64, 3)\n",
            "Train label shape: (10000, 1)\n",
            "Validation shape: (2500, 64, 64, 3)\n",
            "Validation label shape: (2500, 1)\n",
            "y_train_one_hot.shape: (10000, 2)\n",
            "num_classes: 2\n",
            "y_val_one_hot.shape: (2500, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV-gQ09evMwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0caaca25-3d00-47f9-de2e-9f0b486b4e4b"
      },
      "source": [
        "NUM_GENERATION = 3\n",
        "\n",
        "for i in range(NUM_GENERATION):\n",
        "  for client in clients:\n",
        "    \n",
        "    print('\\n---- Generation {}. For Client {} ----'.format(i+1,client.idx))\n",
        "    client.download_model(cloud.send_model())\n",
        "    \n",
        "    # client.train(epoch=cloud.EPOCHS, batch_size=cloud.BATCH_SIZE//cloud.num_clients)\n",
        "    client.train(epochs=1, batch_size=cloud.BATCH_SIZE//cloud.num_clients)\n",
        "    \n",
        "    ''' pass trained weights to aggregator '''\n",
        "    aggregator.get_weights_from_clients(client_idx=client.idx, weights=client.get_weights_from_model())\n",
        "\n",
        "  ''' AFTER CLIENTS ARE DONE TRAINING IN THIS GEN '''\n",
        "  ''' aggregator does his thing and sends the aggregated weights over to cloud '''\n",
        "  aggregator.send_weights_to_cloud(cloud)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- Generation 1. For Client 0 ----\n",
            "313/313 [==============================] - 77s 247ms/step - loss: 0.6920 - accuracy: 0.5249 - val_loss: 0.6987 - val_accuracy: 0.4976\n",
            "-- Received weights from Client 0 --\n",
            "\n",
            "---- Generation 1. For Client 1 ----\n",
            "313/313 [==============================] - 78s 249ms/step - loss: 0.6810 - accuracy: 0.5658 - val_loss: 0.6769 - val_accuracy: 0.5708\n",
            "-- Received weights from Client 1 --\n",
            "--- Aggregating weights... ---\n",
            "-- Saving weights... --\n",
            "\n",
            "---- Generation 2. For Client 0 ----\n",
            "313/313 [==============================] - 79s 252ms/step - loss: 0.6727 - accuracy: 0.5865 - val_loss: 0.6645 - val_accuracy: 0.5996\n",
            "-- Received weights from Client 0 --\n",
            "\n",
            "---- Generation 2. For Client 1 ----\n",
            "313/313 [==============================] - 77s 245ms/step - loss: 0.6585 - accuracy: 0.6133 - val_loss: 0.6470 - val_accuracy: 0.6164\n",
            "-- Received weights from Client 1 --\n",
            "--- Aggregating weights... ---\n",
            "-- Saving weights... --\n",
            "\n",
            "---- Generation 3. For Client 0 ----\n",
            "313/313 [==============================] - 77s 246ms/step - loss: 0.6489 - accuracy: 0.6305 - val_loss: 0.6938 - val_accuracy: 0.5664\n",
            "-- Received weights from Client 0 --\n",
            "\n",
            "---- Generation 3. For Client 1 ----\n",
            "313/313 [==============================] - 77s 247ms/step - loss: 0.6352 - accuracy: 0.6454 - val_loss: 0.6154 - val_accuracy: 0.6636\n",
            "-- Received weights from Client 1 --\n",
            "--- Aggregating weights... ---\n",
            "-- Saving weights... --\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGpqZ00jvOLO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}